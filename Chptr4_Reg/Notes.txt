Traditional use of OLS was to explain relationships between variables (Economists) not to make predictions.
Data Science generally wants to use models to predict new outcomes for unseen data.

DOES NOT prove causation, is just a component in helping the causation case.

For linear regression, RMSE and RSE and not very different especially for large sample sizes.

R-squared: measures the proportion of the variation in the data accounted for in the model.

Choosing variables for a model:
    AIC = 2P + nlog(RSS/n)  P == number of variables, n == number of records
        - goal to minimize AIC and model variables are penalized.
    BIC: similar to AIC with stronger penalty for adding variables
    Mallows CP: variant of AIC developed by Colin Mallows

Stepwise regression:
    start with a full regression and drop variables that dont contribute meaningfully.
        - called backwards elimination
        - forward selection: start with constant model and add variables
    Use AIC to make variable drop decision

no stepwise regression in scikitlearn, but the authors created a package called DMBA which has it.
ridge and lasso regressions and penalized regressions: reducing coefficients by a penalty when they don't contribute to
    explanatory power of model.

weighted regressions:
    - used to analyze complex surveys
    use cases for DS:
        - inverse-variance: weighting observations differently when they have been measured with different precision.
        - Analysis of data where rows represent multiple cases; the weight variable encodes how many original observations
            each row represents.

Confidence interval == uncertainty around regression coefficients
Prediction interval == uncertainty around individual predictions

Drop a category for all factor variables. With an intercept, one of the categories is already known and including
    all categories will result in the "dummy variable trap"
        - Multicollinearity errors

Different encoding techniques besides dummy variables:
    - deviation coding: compares each level against the overall mean
    - polynomial coding: appropriate for ordered factors

Approach to reducing category sizes?
    - Use an existing model
    - group factor variable by the residuals from an initial model (using qcut)

Ordered factor variables can be treated as numerics to preserve the order and used as is.

Multicollinearity is not a concern in non-linear regression methods like trees, clustering, and nearest-neighbors.

influential values: data points which have a high leverage(influence) on the regression.
    - hat-values: values ABOVE 2(P + 1)/n indicates a high leverage data value

Cooks distance: influence defined as combination of leverage and residual size. Usually, values exceeding 4(n - P - 1).

Confidence intervals for predicted values are based on the assumptions about the residuals (normally distributed errors)

For a data scientist, heteroskedasticity may suggest an incomplete model.

Partial residuals help identify the contribution a specific feature has in predicting the outcome and can help identify
    non-linear relationships.

Splines are an alternative, and usually superior alternative to model non-linear relationships.
    - polynomial smoothing between two fixed points.
    - Note ALWAYS better model (best to visualize)

Generalized Additive Models - flexible modeling technique to automatically fit a spline regression without specifying terms.


